{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sklearn_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BMwLRIZbukZm",
        "Il6hiQjyWK6t",
        "ISvFMDUXwhjy",
        "CbE2wsa-w7el"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn7LfzriVtQV"
      },
      "source": [
        "import re\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "corpus = []\n",
        "\n",
        "data = gc.open('train_data').sheet1.get_all_values()\n",
        "data = [item for sublist in data for item in sublist]\n",
        "\n",
        "for line in data:\n",
        "  line = line.replace(\"。\",\"\").replace(\"、\",\"\")\n",
        "  line = re.sub(r\"\\d+\", \"\", line)\n",
        "  corpus.append(line)\n",
        "\n",
        "#informal = 0, polite = 1, formal = 2\n",
        "def label_int(string_label):\n",
        "  if string_label == \"informal\":\n",
        "    return 0\n",
        "  elif string_label == \"polite\":\n",
        "    return 1\n",
        "  else: \n",
        "    return 2\n",
        "\n",
        "label = gc.open('train_label').sheet1.get_all_values()\n",
        "label = [item for sublist in label for item in sublist]\n",
        "\n",
        "y = list(map(lambda x: label_int(x), label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SESSQ7f3V2Rf"
      },
      "source": [
        "!pip install nagisa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAcg3qUWV691",
        "outputId": "01fe54e5-93aa-4f9e-e013-ead8d607c66f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nagisa\n",
        "import pandas as pd\n",
        "\n",
        "def tokenizer_jp(text):\n",
        "  words = nagisa.filter(text, filter_postags=['助詞'])\n",
        "  return words.words\n",
        "\n",
        "# stop_words = ['な', 'と', 'た', 'で', 'は']\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenizer_jp)\n",
        "\n",
        "corpus_tfidf = vectorizer.fit_transform(corpus)\n",
        "\n",
        "frame = pd.DataFrame(corpus_tfidf.toarray(), columns= vectorizer.get_feature_names())\n",
        "print(frame)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus_tfidf, y, test_size = 0.2, random_state = 34)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      あっ  あなた   あの        あり  ありがとう   ある  ...   高く   高層   鳥肌   鶏肉    鷹  黄色く\n",
            "0    0.0  0.0  0.0  0.345933    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1    0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2    0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3    0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4    0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "..   ...  ...  ...       ...    ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "195  0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "196  0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "197  0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "198  0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "199  0.0  0.0  0.0  0.000000    0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[200 rows x 885 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3d1tzBRmIRD"
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMwLRIZbukZm"
      },
      "source": [
        "## **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41xDjKYLWAA4"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(solver = 'liblinear', penalty = 'l2', C =10)\n",
        "\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il6hiQjyWK6t"
      },
      "source": [
        "## **SGD Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8alTyBnHWB2q"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "model = SGDClassifier(loss='hinge', penalty='l2', max_iter=1000, alpha=0.0003, n_iter_no_change=10)\n",
        "\n",
        "# model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISvFMDUXwhjy"
      },
      "source": [
        "## **Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCYZJ6sHwgAR"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB()\n",
        "\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBIgy4de03TM"
      },
      "source": [
        "## **Support Vector Machine (SVM)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZdY-r5A05zx"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbE2wsa-w7el"
      },
      "source": [
        "## **Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhIlr2wsxBCT"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "model = \n",
        "scores = cross_val_score(model, corpus_tfidf, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "print(scores)\n",
        "a = 0\n",
        "for i in scores:\n",
        "  a += i\n",
        "\n",
        "print(a/10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvC0sBGCWVDc"
      },
      "source": [
        "# **train : test = 8 : 2**\n",
        "100 sentences accuracy = 0.8\n",
        "\n",
        "200 sentences accuracy = 0.85"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax76nGRjWKFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16fe79bf-11dc-46bd-eb54-b779338573cf"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(accuracy)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.925\n",
            "[[14  1  0]\n",
            " [ 2 14  0]\n",
            " [ 0  0  9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90        15\n",
            "           1       0.93      0.88      0.90        16\n",
            "           2       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           0.93        40\n",
            "   macro avg       0.94      0.94      0.94        40\n",
            "weighted avg       0.93      0.93      0.93        40\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZaNRt83EAHF"
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(model, open('model.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at0G3rowNWQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}